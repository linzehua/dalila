
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>DALILA (DictionAry LearnIng LibrAry) &#8212; DALILA 1.0 documentation</title>
    
    <link rel="stylesheet" href="_static/slipGuru.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '1.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Quick start tutorial" href="tutorial.html" />
<meta name="keywords"
      content="SlipGuru, 'University of Genoa', statistical learning,
      computational biology, Python, distributed, computing, parallel" />
<map id="logosmap" name="logosmap">
    <area shape="rect" alt="SlipGURU" title="SlipGURU" coords="0,0,89,112"
          href="http://slipguru.disi.unige.it/" />
    <area shape="rect" alt="Dipartimento di Informatica e Scienze dell'Informazione"
          title="Dipartimento di Informatica e Scienze dell'Informazione"
          coords="95,4,200,34" href="http://www.disi.unige.it/" />
    <area shape="rect" alt="Università Degli Studi di Genova"
          title="Università Degli Studi di Genova" coords="124,48,171,107"
          href="http://www.unige.it/" />
</map>

  </head>
  <body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="tutorial.html" title="Quick start tutorial"
             accesskey="N">next</a> |</li>
        <li class="nav-item nav-item-0"><a href="#">DALILA 1.0 documentation</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="dalila-dictionary-learning-library">
<h1>DALILA (DictionAry LearnIng LibrAry)<a class="headerlink" href="#dalila-dictionary-learning-library" title="Permalink to this headline">¶</a></h1>
<p><strong>DALILA</strong> is a Dictionary Learning Library whose purpose is to find a
decomposition of an input matrix <strong>X</strong> into two other matrices <strong>D</strong> and <strong>C</strong>
which are respectively the <em>dictionary</em> which contains the basic atoms and the
<em>coefficients</em> that are weights for the atoms. The linear combination of
the atoms weighted with the coefficients give an approximation of the original
signal.
It offers also methods for Representation Learning which is a related method
that given the dictionary <strong>D</strong> and the signal matrix <strong>X</strong> finds the
coefficients.</p>
<p>We propose a generic optimization algorithm that can optimize the functional
with different penalties both on the dictionary and on the coefficients.</p>
<p>The library allows to run some of its computationally expensive parts in parallel
on the same machine or distributing the tasks with dask
(<a class="reference external" href="http://dask.pydata.org/en/latest/index.html">http://dask.pydata.org/en/latest/index.html</a>).</p>
<div class="section" id="user-documentation">
<h2>User documentation<a class="headerlink" href="#user-documentation" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorial.html">Quick start tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="tutorial.html#installation-from-sources">Installation from sources</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial.html#examples">Examples</a></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="api">
<span id="id1"></span><h3>API<a class="headerlink" href="#api" title="Permalink to this headline">¶</a></h3>
<div class="toctree-wrapper compound">
</div>
<div class="section" id="module-dalila.dictionary_learning">
<span id="classes"></span><h4>Classes<a class="headerlink" href="#module-dalila.dictionary_learning" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="dalila.dictionary_learning.DictionaryLearning">
<em class="property">class </em><code class="descclassname">dalila.dictionary_learning.</code><code class="descname">DictionaryLearning</code><span class="sig-paren">(</span><em>k</em>, <em>dict_penalty=None</em>, <em>coeff_penalty=None</em>, <em>dict_normalization=0</em>, <em>non_negativity=’none’</em>, <em>random_state=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalila/dictionary_learning.html#DictionaryLearning"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalila.dictionary_learning.DictionaryLearning" title="Permalink to this definition">¶</a></dt>
<dd><p>An estimator for dictionary learning based on alternating prox methods.</p>
<p>This estimator optimises a functional of the following form:</p>
<p>(1/2)||X - CD||_F^2 + phi(D) + psi(C)</p>
<p>where the norm of the reconstruction error is a Frobenious matrix norm
and the functions phi and psi are a combination of penalties that act
row-wise on both the matrices.
It is possible to specify which kind of penalties you want to use on
the matrices, if there must be a non-negativity constraint and if the
atoms in the dictionary must have norm equal to one.</p>
<dl class="docutils">
<dt>k: int</dt>
<dd>Number of atoms in which decompose the input matrix.</dd>
<dt>dict_penalty <span class="classifier-delimiter">:</span> <span class="classifier">a sub-class of Penalty in penalty.py file,</span></dt>
<dd>optional
It is applied on the dictionary and it can be L0Penalty,
L1Penalty, L2Penalty, ElasticNetPenalty</dd>
<dt>coeff_penalty: sa sub-class of Penalty class in penalty.py file,</dt>
<dd>optional
It is applied on the coefficients and it can be L0Penalty,
L1Penalty, L2Penalty, ElasticNetPenalty</dd>
<dt>dict_normalization: int, optional</dt>
<dd>If different than zero the atoms are normalized to have l2-norm
equal to 1.</dd>
<dt>non_negativity: string, optional</dt>
<dd>If ‘none’ (default) the atoms and the coefficients can be both
non negative.
If ‘both’ non-negativity is applied on both matrices.
if ‘coeff’ non-negativity is applied only on the matrix of the
coefficients.</dd>
<dt>random_state: RandomState or int</dt>
<dd>Seed to be use to initialise np.random.RandomState. If None each
time RandomState is randomly initialised.</dd>
</dl>
<dl class="method">
<dt id="dalila.dictionary_learning.DictionaryLearning.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>x</em>, <em>y=None</em>, <em>n_iter=20000</em>, <em>backtracking=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalila/dictionary_learning.html#DictionaryLearning.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalila.dictionary_learning.DictionaryLearning.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Function that fits the estimator on the matrix X.</p>
<p>This function finds the decomposition in dictionary and coefficients
with an alternating proximal gradient algorithm. It iteratively
computes the gradient on one of the two matrices by keeping the other
fixed and then apply the prox operator of the specified penalty.
If the flat non-negative is set it project the rows of the matrices to
the positive space, while if the dict_normalization flag is set the
rows of the dictionaries are normalised to have l2-norm equal to 1.</p>
<dl class="docutils">
<dt>x <span class="classifier-delimiter">:</span> <span class="classifier">array-like or sparse matrix shape =  (n_samples, n_features)</span></dt>
<dd>The matrix to decompose.</dd>
<dt>y:</dt>
<dd>Inserted for compatibility with sklearn library.</dd>
<dt>n_iter: int, optional</dt>
<dd>Maximum number of iterations the algorithm does before stopping.</dd>
<dt>backtracking: bool, optional</dt>
<dd>If True a procedure of backtracking is done on the step in order
to avoid an increasing in the objective function.</dd>
</dl>
<p>self : object</p>
</dd></dl>

<dl class="method">
<dt id="dalila.dictionary_learning.DictionaryLearning.reconstruction_error">
<code class="descname">reconstruction_error</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalila/dictionary_learning.html#DictionaryLearning.reconstruction_error"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalila.dictionary_learning.DictionaryLearning.reconstruction_error" title="Permalink to this definition">¶</a></dt>
<dd><dl class="docutils">
<dt>float:</dt>
<dd>The reconstruction error for the current decomposition of the
matrix. If no decomposition was run infinity is returned.
A lower reconstruction error corresponds to a better approximation
of the input data.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="dalila.dictionary_learning.DictionaryLearning.objective_function_value">
<code class="descname">objective_function_value</code><span class="sig-paren">(</span><em>x=None</em>, <em>d=None</em>, <em>c=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalila/dictionary_learning.html#DictionaryLearning.objective_function_value"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalila.dictionary_learning.DictionaryLearning.objective_function_value" title="Permalink to this definition">¶</a></dt>
<dd><dl class="docutils">
<dt>x <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_samples, n_features)</span></dt>
<dd>The matrix to be decomposed.</dd>
<dt>d: array_like, shape=(n_atoms, n_features)</dt>
<dd>The dictionary.</dd>
<dt>c: array-like, shape=(n_samples, n_atoms)</dt>
<dd>The matrix of coefficients</dd>
</dl>
<p>If one of the three is None the internal decomposition is taken, if no
decomposition is available NaN is returned.</p>
<dl class="docutils">
<dt>float:</dt>
<dd>The value of the objective function.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="dalila.dictionary_learning.DictionaryLearning.decomposition">
<code class="descname">decomposition</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalila/dictionary_learning.html#DictionaryLearning.decomposition"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalila.dictionary_learning.DictionaryLearning.decomposition" title="Permalink to this definition">¶</a></dt>
<dd><dl class="docutils">
<dt>C: array-like, shape = (n_samples, k)</dt>
<dd>The learnt coefficients.</dd>
<dt>D: array_like, shape = (k, n_features)</dt>
<dd>The learnt dictionary.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="dalila.dictionary_learning.DictionaryLearning.score">
<code class="descname">score</code><span class="sig-paren">(</span><em>*args</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalila/dictionary_learning.html#DictionaryLearning.score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalila.dictionary_learning.DictionaryLearning.score" title="Permalink to this definition">¶</a></dt>
<dd><dl class="docutils">
<dt><a href="#id2"><span class="problematic" id="id3">*</span></a>args: optional</dt>
<dd>Introduced for compatibility with sklearn GridSearchCV</dd>
</dl>
<dl class="docutils">
<dt>float</dt>
<dd>The score of the current decomposition. BIC value computed as
-log(k)*log(n_samples) - 2.3*(self.objective_function_value())
the highest is the score and better the decomposition is.</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="dalila.dictionary_learning.StabilityDictionaryLearning">
<em class="property">class </em><code class="descclassname">dalila.dictionary_learning.</code><code class="descname">StabilityDictionaryLearning</code><span class="sig-paren">(</span><em>k</em>, <em>dict_penalty=None</em>, <em>coeff_penalty=None</em>, <em>dict_normalization=1</em>, <em>non_negativity=’none’</em>, <em>random_state=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalila/dictionary_learning.html#StabilityDictionaryLearning"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalila.dictionary_learning.StabilityDictionaryLearning" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimator for dictionary learning (DL) based on prox methods and clusters.</p>
<p>This estimator optimises a functional of the following form:</p>
<p>(1/2)||X - CD||_F^2 + phi(D) + psi(C)</p>
<p>where the norm of the reconstruction error is a Frobenious matrix norm
and the functions phi and psi are a combination of penalties that act
row-wise on both the matrices.
It is possible to specify which kind of penalties you want to use on
the matrices, if there must be a non-negativity constraint and if the
atoms in the dictionary must have norm equal to one.</p>
<p>The algorithm performs many decomposition using DL and after each
iteration it clusters the atoms obtained in every iteration in order to
have final atoms that are stable w.r.t to data noise.</p>
<p>For more details see Alexandrov et al, Cell 2013.</p>
<dl class="docutils">
<dt>k: int</dt>
<dd>Number of atoms in which decompose the input matrix.</dd>
<dt>dict_penalty <span class="classifier-delimiter">:</span> <span class="classifier">a sub-class of Penalty in penalty.py file,</span></dt>
<dd>optional
It is applied on the dictionary and it can be L0Penalty,
L1Penalty, L2Penalty, ElasticNetPenalty</dd>
<dt>coeff_penalty: sa sub-class of Penalty class in penalty.py file,</dt>
<dd>optional
It is applied on the coefficients and it can be L0Penalty,
L1Penalty, L2Penalty, ElasticNetPenalty</dd>
<dt>dict_normalization: int, optional</dt>
<dd>If different than zero the atoms are normalized to have l2-norm
equal to 1.</dd>
<dt>non_negativity: string, optional</dt>
<dd>If ‘none’ (default) the atoms and the coefficients can be both
non negative.
If ‘both’ non-negativity is applied on both matrices.
if ‘coeff’ non-negativity is applied only on the matrix of the
coefficients.</dd>
<dt>random_state: RandomState or int</dt>
<dd>Seed to be use to initialise np.random.RandomState. If None each
time RandomState is randomly initialised.</dd>
</dl>
<dl class="method">
<dt id="dalila.dictionary_learning.StabilityDictionaryLearning.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>x</em>, <em>y=None</em>, <em>backtracking=0</em>, <em>n_iter=20000</em>, <em>epsilon=0.0001</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalila/dictionary_learning.html#StabilityDictionaryLearning.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalila.dictionary_learning.StabilityDictionaryLearning.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Function that fits the estimator on the matrix X.</p>
<p>This function finds the decomposition in dictionary and coefficients
with an alternating proximal gradient algorithm. It iteratively
computes the gradient on one of the two matrices by keeping the
other fixed and then apply the prox operator of the specified
penalty. If the flat non-negative is set it project the rows of the
matrices to the positive space, while if the dict_normalization flag
is set the rows of the dictionaries are normalised to have l2-norm
equal to 1.</p>
<dl class="docutils">
<dt>x <span class="classifier-delimiter">:</span> <span class="classifier">array-like or sparse matrix shape =  (n_samples, n_features)</span></dt>
<dd>The matrix to decompose.</dd>
<dt>y:</dt>
<dd>Inserted for compatibility with sklearn library.</dd>
<dt>n_iter: int, optional</dt>
<dd>Maximum number of iterations the algorithm does before stopping.</dd>
<dt>backtracking: bool, optional</dt>
<dd>If True a procedure of backtracking is done on the step in order
to avoid an increasing in the objective function.</dd>
<dt>epsilon: float, optional</dt>
<dd>The difference between the each iteration of DL at which the
algorithm stops to return the solution.</dd>
</dl>
<p>self : object</p>
</dd></dl>

<dl class="method">
<dt id="dalila.dictionary_learning.StabilityDictionaryLearning.decomposition">
<code class="descname">decomposition</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalila/dictionary_learning.html#StabilityDictionaryLearning.decomposition"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalila.dictionary_learning.StabilityDictionaryLearning.decomposition" title="Permalink to this definition">¶</a></dt>
<dd><dl class="docutils">
<dt>C: array-like, shape = (n_samples, k)</dt>
<dd>The learnt mean coefficients.</dd>
<dt>D: array_like, shape = (k, n_features)</dt>
<dd>The learnt mean dictionary, the centroids of the clusters of
atoms.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="dalila.dictionary_learning.StabilityDictionaryLearning.stability">
<code class="descname">stability</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalila/dictionary_learning.html#StabilityDictionaryLearning.stability"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalila.dictionary_learning.StabilityDictionaryLearning.stability" title="Permalink to this definition">¶</a></dt>
<dd><dl class="docutils">
<dt>float</dt>
<dd>The mean stability obtained with the clusterization. Where for
stability we mean the average silhouette over all the clusters.</dd>
</dl>
</dd></dl>

</dd></dl>

<span class="target" id="module-dalila.representation_learning"></span><dl class="class">
<dt id="dalila.representation_learning.RepresentationLearning">
<em class="property">class </em><code class="descclassname">dalila.representation_learning.</code><code class="descname">RepresentationLearning</code><span class="sig-paren">(</span><em>penalty=None</em>, <em>non_negativity=0</em>, <em>random_state=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalila/representation_learning.html#RepresentationLearning"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalila.representation_learning.RepresentationLearning" title="Permalink to this definition">¶</a></dt>
<dd><p>An estimator for finding coefficients given the dictionary.</p>
<p>This estimator, given X and D, optimises a functional of the
following form:</p>
<p>(1/2)||X - CD||_F^2 +  psi(C)</p>
<p>where the norm of the reconstruction error is a Frobenious matrix norm
and the function psi is a combination of penalties that act
row-wise on both the matrices.
It is possible to specify which kind of penalties you want to use on
the matrices, if there must be a non-negativity constraint and if the
atoms in the dictionary must have norm equal to one.</p>
<dl class="docutils">
<dt>penalty: a sub-class of Penalty class in penalty.py file, optional</dt>
<dd>It is applied on the coefficients and it can be
- L0Penalty
- L1Penalty
- L2Penalty
- ElasticNetPenalty
- GroupLassoPenalty
- LInfPenalty</dd>
<dt>non_negativity: bool, optional</dt>
<dd>If true the coefficients are projected to the non-negative
space.</dd>
<dt>random_state: RandomState or int</dt>
<dd>Seed to be use to initialise np.random.RandomState. If None each
time RandomState is randomly initialised.</dd>
</dl>
<dl class="method">
<dt id="dalila.representation_learning.RepresentationLearning.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>x</em>, <em>d</em>, <em>backtracking=0</em>, <em>n_iter=20000</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalila/representation_learning.html#RepresentationLearning.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalila.representation_learning.RepresentationLearning.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Function that fits the estimator on the matrices X and D.</p>
<p>This function finds the coefficients with a proximal gradient
algorithm. It computes the gradient on the reconstruction part of the
functional and then applies the prox operator of the specified penalty.
If the flat non-negative is set it project the rows of the matrices to
the positive space.</p>
<dl class="docutils">
<dt>x <span class="classifier-delimiter">:</span> <span class="classifier">array-like or sparse matrix shape =  (n_samples, n_features)</span></dt>
<dd>The matrix to decompose.</dd>
<dt>d: array_like or sparse matrix, shape= (n_atoms, n_features)</dt>
<dd>The dictionary.</dd>
<dt>backtracking: bool, optional</dt>
<dd>If True a procedure of backtracking is done on the step in order
to avoid an increasing in the objective function.</dd>
<dt>n_iter: int, optional</dt>
<dd>Maximum number of iteration the minimization algorithm can
perform.</dd>
</dl>
<p>self : object</p>
</dd></dl>

<dl class="method">
<dt id="dalila.representation_learning.RepresentationLearning.reconstruction_error">
<code class="descname">reconstruction_error</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalila/representation_learning.html#RepresentationLearning.reconstruction_error"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalila.representation_learning.RepresentationLearning.reconstruction_error" title="Permalink to this definition">¶</a></dt>
<dd><dl class="docutils">
<dt>float:</dt>
<dd>The reconstruction error for the current decomposition of the
matrix. If no decomposition was run infinity is returned.
A lower reconstruction error corresponds to a better approximation
of the input data.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="dalila.representation_learning.RepresentationLearning.objective_function_value">
<code class="descname">objective_function_value</code><span class="sig-paren">(</span><em>x=None</em>, <em>d=None</em>, <em>c=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalila/representation_learning.html#RepresentationLearning.objective_function_value"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalila.representation_learning.RepresentationLearning.objective_function_value" title="Permalink to this definition">¶</a></dt>
<dd><dl class="docutils">
<dt>x <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_samples, n_features)</span></dt>
<dd>The matrix to be decomposed.</dd>
<dt>d: array_like, shape=(n_atoms, n_features)</dt>
<dd>The dictionary.</dd>
<dt>c: array-like, shape=(n_samples, n_atoms)</dt>
<dd>The matrix of coefficients</dd>
</dl>
<p>If one of the three is None the internal decomposition is taken, if no
decomposition is available NaN is returnes.</p>
<dl class="docutils">
<dt>float:</dt>
<dd>The value of the objective function.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="dalila.representation_learning.RepresentationLearning.coefficients">
<code class="descname">coefficients</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalila/representation_learning.html#RepresentationLearning.coefficients"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalila.representation_learning.RepresentationLearning.coefficients" title="Permalink to this definition">¶</a></dt>
<dd><dl class="docutils">
<dt>C: array-like, shape = (n_samples, k)</dt>
<dd>The learnt coefficients.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="dalila.representation_learning.RepresentationLearning.score">
<code class="descname">score</code><span class="sig-paren">(</span><em>*args</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalila/representation_learning.html#RepresentationLearning.score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalila.representation_learning.RepresentationLearning.score" title="Permalink to this definition">¶</a></dt>
<dd><dl class="docutils">
<dt><a href="#id4"><span class="problematic" id="id5">*</span></a>args: optional</dt>
<dd>Introduced for compatibility with sklearn GridSearchCV</dd>
</dl>
<dl class="docutils">
<dt>float</dt>
<dd><dl class="first docutils">
<dt>The score of the current decomposition. BIC value computed as</dt>
<dd>k(log(n_samples)-log(2pi)) - 2*(||X - CD||)</dd>
</dl>
<p class="last">the highest is the score and better the decomposition is.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-dalila.parameters_research">
<span id="parameter-research-utils"></span><h4>Parameter research utils<a class="headerlink" href="#module-dalila.parameters_research" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="dalila.parameters_research.tune_parameters_DL">
<code class="descclassname">dalila.parameters_research.</code><code class="descname">tune_parameters_DL</code><span class="sig-paren">(</span><em>X</em>, <em>estimator=None</em>, <em>analysis=3</em>, <em>non_negative=’none’</em>, <em>distributed=0</em>, <em>scheduler_host=”</em>, <em>max_k=None</em>, <em>dict_penalty_range=(0.0001</em>, <em>1</em>, <em>10)</em>, <em>coeff_penalty_range=(0.0001</em>, <em>1</em>, <em>10)</em>, <em>random_state=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalila/parameters_research.html#tune_parameters_DL"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalila.parameters_research.tune_parameters_DL" title="Permalink to this definition">¶</a></dt>
<dd><p>Parameters tuner.</p>
<p>It tunes the parameters of a dictionary learning estimator using 3-splits
monte carlo sampling cross validation.</p>
<dl class="docutils">
<dt>X: array-like, shape=(n_samples, n_features)</dt>
<dd>The matrix to decompose and analyse.</dd>
<dt>estimator: DictionaryLearning class, optional</dt>
<dd>The estimator you want to use to analyse the matrix. If None only the
research on the best number of atoms will be done.</dd>
<dt>analysis: int, optional</dt>
<dd><p class="first">The type of tuning you want to perform.
- 0: tune together number of atoms and dictionary penalty and then the</p>
<blockquote>
<div>coefficients penalty</div></blockquote>
<ul class="last simple">
<li><dl class="first docutils">
<dt>1: tune only the penalties and take the number of atoms as specified</dt>
<dd>in the estimator</dd>
</dl>
</li>
<li>2: tune only the number of atoms</li>
<li>3: tune all together, number of atoms and penalties</li>
</ul>
</dd>
<dt>non_negative: string, optional</dt>
<dd>If “none” no negativity is imposed on the decomposition, if “coeff”
only negativity on the coefficient is imposed. If “both” negativiy is
on both decomposition matrices.</dd>
<dt>distributed: int, optional</dt>
<dd>If 0 the parameters research will be executed in parallel on the
computer the script is launched.
If 1 the parameters research will be executed sequentially.
If 2 the parameters research will be distributed on multiple machines
connected by dask. In this case also scheduler_host must be speficied.</dd>
<dt>scheduler_host: string, optional</dt>
<dd>If distributed=2 it is necessary to specify the scheduler of the dask
network. The string must be “ip_address:port”, for example:
“10.251.61.226:8786”</dd>
<dt>max_k = int, optional</dt>
<dd>The maximum number of atoms to try when you search for the right k.
If None max_k will be computed as int(min(p, 0.75 * n) / 2)</dd>
<dt>dict_penalty_range: float tuple, optional (low, high, number)</dt>
<dd>It gives the interval in which tune the dictionary penalty and the
number of values to try.</dd>
<dt>coeff_penalty_range: float tuple, optional (low, high, number)</dt>
<dd>It gives the interval in which tune the coefficient penalty and the
number of values to try.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
</dl>
<p>GridSearchCV
The resulting GridSearch.</p>
</dd></dl>

<dl class="function">
<dt id="dalila.parameters_research.tune_parameters_RL">
<code class="descclassname">dalila.parameters_research.</code><code class="descname">tune_parameters_RL</code><span class="sig-paren">(</span><em>X</em>, <em>D</em>, <em>estimator</em>, <em>non_negative=0</em>, <em>distributed=0</em>, <em>scheduler_host=”</em>, <em>coeff_penalty_range=(0.0001</em>, <em>1</em>, <em>10)</em>, <em>random_state=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalila/parameters_research.html#tune_parameters_RL"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalila.parameters_research.tune_parameters_RL" title="Permalink to this definition">¶</a></dt>
<dd><p>Parameters tuner.</p>
<p>It tunes the parameters of a representations learning estimator using
3-splits monte carlo sampling cross validation.</p>
<dl class="docutils">
<dt>X: array-like, shape=(n_samples, n_features)</dt>
<dd>The matrix to decompose and analyse.</dd>
<dt>D: array-like, shape=(n_atoms, n_features)</dt>
<dd>The dictionary.</dd>
<dt>estimator: RepresentationLearning class, optional</dt>
<dd>The estimator you want to use to analyse the matrix.</dd>
</dl>
<p>non_negative: boolean, optional</p>
<dl class="docutils">
<dt>distributed: int, optional</dt>
<dd>If 0 the parameters research will be executed in parallel on the
computer the script is launched.
If 1 the parameters research will be executed sequentially.
If 2 the parameters research will be distributed on multiple machines
connected by dask. In this case also scheduler_host must be speficied.</dd>
<dt>scheduler_host: string, optional</dt>
<dd>If distributed=2 it is necessary to specify the scheduler of the dask
network. The string must be “ip_address:port”, for example:
“10.251.61.226:8786”</dd>
<dt>coeff_penalty_range: float tuple, optional (low, high, number)</dt>
<dd>It gives the interval in which tune the coefficient penalty and the
number of values to try.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
</dl>
<p>GridSearchCV
The resulting GridSearch.</p>
</dd></dl>

</div>
<div class="section" id="module-dalila.plot">
<span id="plotting-functions"></span><h4>Plotting functions<a class="headerlink" href="#module-dalila.plot" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="dalila.plot.plot_dictionary_atoms">
<code class="descclassname">dalila.plot.</code><code class="descname">plot_dictionary_atoms</code><span class="sig-paren">(</span><em>dictionary</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalila/plot.html#plot_dictionary_atoms"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalila.plot.plot_dictionary_atoms" title="Permalink to this definition">¶</a></dt>
<dd><p>It plots the atoms composing the dictionary.</p>
<p>dictionary: array-like, shape=(n_atoms, n_features)</p>
</dd></dl>

<dl class="function">
<dt id="dalila.plot.plot_atoms_as_histograms">
<code class="descclassname">dalila.plot.</code><code class="descname">plot_atoms_as_histograms</code><span class="sig-paren">(</span><em>dictionary</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalila/plot.html#plot_atoms_as_histograms"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalila.plot.plot_atoms_as_histograms" title="Permalink to this definition">¶</a></dt>
<dd><p>It plots the atoms composing the dictionary as histograms.</p>
<p>dictionary: array_like, shape=(n_atoms, n_features)</p>
</dd></dl>

</div>
<div class="section" id="module-dalila.dataset_generator">
<span id="dataset-generator"></span><h4>Dataset generator<a class="headerlink" href="#module-dalila.dataset_generator" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="dalila.dataset_generator.group_lasso_dataset_generator">
<code class="descclassname">dalila.dataset_generator.</code><code class="descname">group_lasso_dataset_generator</code><span class="sig-paren">(</span><em>n_samples=100</em>, <em>n_features=100</em>, <em>gaussian_noise=0.5</em>, <em>random_state=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalila/dataset_generator.html#group_lasso_dataset_generator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalila.dataset_generator.group_lasso_dataset_generator" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates synthetic data for group lasso tests.</p>
<p>This function generates a matrix generated from 7 basic atoms, grouped
as [0, 1, 3], [2, 4, 5], linearly combined with random weights.
A certain level of gaussian noise is added to the signal.</p>
<blockquote>
<div>n_samples: int, optional</div></blockquote>
<p>Number of samples for the output matrix.</p>
<dl class="docutils">
<dt>n_features: int, optional</dt>
<dd>Number of features the output matrix must have.</dd>
<dt>gaussian_noise: float, optional</dt>
<dd>The level of noise to add to the synthetic data.</dd>
<dt>random_state: RandomState or int, optional</dt>
<dd>RandomState or seed used to generate RandomState for the
reproducibility of data. If None each time RandomState is randomly
initialised.</dd>
</dl>
<dl class="docutils">
<dt>array_like, shape=(n_samples, n_features)</dt>
<dd>Generated matrix of data</dd>
<dt>array_like, shape=(n_samples, 7)</dt>
<dd>Coefficients</dd>
<dt>array_like, shape=(7, n_features)</dt>
<dd>Dictionary</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="dalila.dataset_generator.sparse_signal_generator">
<code class="descclassname">dalila.dataset_generator.</code><code class="descname">sparse_signal_generator</code><span class="sig-paren">(</span><em>n_samples</em>, <em>n_features</em>, <em>frequencies</em>, <em>support_atoms</em>, <em>shift=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalila/dataset_generator.html#sparse_signal_generator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalila.dataset_generator.sparse_signal_generator" title="Permalink to this definition">¶</a></dt>
<dd><p>The following function generates signals using sawtooth and sin</p>
<dl class="docutils">
<dt>n_samples <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>number of signals to be generated</dd>
<dt>n_features <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>length of the time series (number of points)</dd>
<dt>frequencies :</dt>
<dd>number of frequencies (to be used for the def of atoms)</dd>
<dt>support_atoms:</dt>
<dd>qualcosa</dd>
<dt>shift :</dt>
<dd>if true shifted atoms, else fixed</dd>
</dl>
<dl class="docutils">
<dt>multichannel_matrix <span class="classifier-delimiter">:</span> <span class="classifier">np.array(n_features, n_samples)</span></dt>
<dd>matrix of signals</dd>
<dt>atoms_matrix <span class="classifier-delimiter">:</span> <span class="classifier">np.array(n_features, number_of_atoms)</span></dt>
<dd>matrix of signals</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="dalila.dataset_generator.synthetic_data_non_negative">
<code class="descclassname">dalila.dataset_generator.</code><code class="descname">synthetic_data_non_negative</code><span class="sig-paren">(</span><em>gaussian_noise=1</em>, <em>random_state=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalila/dataset_generator.html#synthetic_data_non_negative"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalila.dataset_generator.synthetic_data_non_negative" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates synthetic non-negative data for dictionary learning tests.</p>
<p>This function generates a matrix generated from 7 basic atoms linearly
combined with random weights sparse over the atoms. A certain level of
gaussian noise is added to the signal.</p>
<dl class="docutils">
<dt>gaussian_noise: float, optional</dt>
<dd>The level of noise to add to the synthetic data.</dd>
<dt>random_state: RandomState or int, optional</dt>
<dd>RandomState or seed used to generate RandomState for the
reproducibility of data. If None each time RandomState is randomly
initialised.</dd>
</dl>
<dl class="docutils">
<dt>array_like, shape=(80, 96)</dt>
<dd>Generated matrix of data</dd>
<dt>array_like, shape=(80, 7)</dt>
<dd>Coefficients</dd>
<dt>array_like, shape=(7, 96)</dt>
<dd>Dictionary</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="dalila.dataset_generator.synthetic_data_negative">
<code class="descclassname">dalila.dataset_generator.</code><code class="descname">synthetic_data_negative</code><span class="sig-paren">(</span><em>n_samples=100</em>, <em>n_features=60</em>, <em>gaussian_noise=1</em>, <em>random_state=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalila/dataset_generator.html#synthetic_data_negative"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalila.dataset_generator.synthetic_data_negative" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates synthetic data for dictionary learning tests.</p>
<p>This function generates a matrix generated from 10 basic atoms linearly
combined with random weights sparse over the atoms. A certain level of
gaussian noise is added to the signal.</p>
<dl class="docutils">
<dt>n_samples: int, optional</dt>
<dd>Number of samples for the output matrix.</dd>
<dt>n_features: int, optional</dt>
<dd>Number of features the output matrix must have.</dd>
<dt>gaussian_noise: float, optional</dt>
<dd>The level of noise to add to the synthetic data.</dd>
<dt>random_state: RandomState or int, optional</dt>
<dd>RandomState or seed used to generate RandomState for the
reproducibility of data. If None each time RandomState is randomly
initialised.</dd>
</dl>
<dl class="docutils">
<dt>array_like, shape=(n_samples, number_of_features)</dt>
<dd>Generated matrix of data</dd>
<dt>array_like, shape=(n_samples, 5)</dt>
<dd>Coefficients</dd>
<dt>array_like, shape=(5, n_features)</dt>
<dd>Dictionary</dd>
</dl>
</dd></dl>

</div>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
   
      <h3>Download</h3>
      <p>Current version: <b>1.0</b></p>
      <p>Get DALILA from the
         <a href="http://pypi.python.org/pypi/DALILA">Python Package Index</a>,
         or install it with:
      </p>
      <pre>pip install --upgrade DALILA</pre>
      <p> or clone it from our <a href="https://github.com/slipguru/DALILA">GitHub</a> repository: </p>
      <pre>git clone https://github.com/slipguru/DALILA</pre>
      <!--or:-->
      <!--<pre>easy_install -U DALILA</pre>-->
<!--      <p>Latest
         <a href="https://bitbucket.org/slipguru/dalila/downloads/DALILA.pdf">
            documentation in pdf
         </a>
         is also available. -->
      </p>
   

   
   

  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/index.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
    
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
    <p class="logo">
       <img class="logo" src="_static/logos.png" alt="Logos" usemap="#logosmap" />
    </p>

        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="tutorial.html" title="Quick start tutorial"
             >next</a> |</li>
        <li class="nav-item nav-item-0"><a href="#">DALILA 1.0 documentation</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2017 Veronica Tozzo - Vanessa D&#39;Amario - Annalisa Barla.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.6.2.
    </div>
  </body>
</html>